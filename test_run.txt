==== Epoch 9 ====
  Epoch 9 (train) took 815.67s [processed 505 samples/second]
 Training Loss: 0.146924
  Epoch 9 (valid) took 20.63s [processed 1114 samples/second]
 Validation:  Loss: 2.861277 | MRR: 0.608425
2020-01-07 02:16:35.308543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-07 02:16:35.308589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-07 02:16:35.308602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-07 02:16:35.308620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-07 02:16:35.308713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7027 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Test-All MRR (bs=1,000):  0.659
FuncNameTest-All MRR (bs=1,000):  0.641
Validation-All MRR (bs=1,000):  0.613
Test-python MRR (bs=1,000):  0.659
FuncNameTest-python MRR (bs=1,000):  0.641
Validation-python MRR (bs=1,000):  0.613

wandb: Waiting for W&B process to finish, PID 1046
wandb: Program ended successfully.
wandb: You can sync this run to the cloud by running: 
wandb: wandb sync wandb/dryrun-20200106_235319-xgvu3646








sudo script/console
[sudo] password for larumuga: 
Mon Jan  6 14:13:36 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.44       Driver Version: 440.44       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |
|  0%   36C    P8     9W / 200W |    392MiB /  8118MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
root@daedalus:/home/dev/src# python train.py --testrun
wandb: W&B is a tool that helps track and visualize machine learning experiments
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 2
wandb: You chose 'Use an existing W&B account'
wandb: You can find your API key in your browser here: https://app.wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Started W&B process version 0.8.12 with PID 29
wandb: Wandb version 0.8.19 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Local directory: wandb/run-20200106_141406-21aolhym
wandb: Syncing run neuralbowmodel-2020-01-06-14-14-06: https://app.wandb.ai/laksh47/CodeSearchNet/runs/21aolhym
wandb: Run `wandb off` to turn off syncing.

2020-01-06 14:17:05.359558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-06 14:17:05.360393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7465
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.45GiB
2020-01-06 14:17:05.360414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-06 14:17:06.537060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-06 14:17:06.537100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-06 14:17:06.537108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-06 14:17:06.537270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7191 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Tokenizing and building vocabulary for code snippets and queries.  This step may take several hours.
WARNING:tensorflow:From /home/dev/src/models/model.py:299: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Starting training run neuralbowmodel-2020-01-06-14-14-06 of model NeuralBoWModel with following hypers:
{'code_token_vocab_size': 10000, 'code_token_vocab_count_threshold': 10, 'code_token_embedding_size': 128, 'code_use_subtokens': False, 'code_mark_subtoken_end': False, 'code_max_num_tokens': 200, 'code_use_bpe': True, 'code_pct_bpe': 0.5, 'code_nbow_pool_mode': 'weighted_mean', 'query_token_vocab_size': 10000, 'query_token_vocab_count_threshold': 10, 'query_token_embedding_size': 128, 'query_use_subtokens': False, 'query_mark_subtoken_end': False, 'query_max_num_tokens': 30, 'query_use_bpe': True, 'query_pct_bpe': 0.5, 'query_nbow_pool_mode': 'weighted_mean', 'batch_size': 1000, 'optimizer': 'Adam', 'seed': 0, 'dropout_keep_rate': 0.9, 'learning_rate': 0.01, 'learning_rate_code_scale_factor': 1.0, 'learning_rate_query_scale_factor': 1.0, 'learning_rate_decay': 0.98, 'momentum': 0.85, 'gradient_clip': 1, 'loss': 'cosine', 'margin': 1, 'max_epochs': 2, 'patience': 5, 'fraction_using_func_name': 0.1, 'min_len_func_name_for_query': 12, 'query_random_token_frequency': 0.0}
Loading training and validation data.
Begin Training.
Training on 30000 go, 30000 java, 30000 javascript, 30000 php, 30000 python, 30000 ruby samples.
Validating on 8253 javascript, 26015 php, 2209 ruby, 23107 python, 15328 java, 14242 go samples.
==== Epoch 0 ====
  Epoch 0 (train) took 19.79s [processed 9096 samples/second]
 Training Loss: 1.030792
  Epoch 0 (valid) took 6.89s [processed 12924 samples/second]
 Validation:  Loss: 1.008378 | MRR: 0.116060
  Best result so far -- saved model as '/home/dev/resources/saved_models/neuralbowmodel-2020-01-06-14-14-06_model_best.pkl.gz'.
==== Epoch 1 ====
  Epoch 1 (train) took 18.41s [processed 9778 samples/second]
 Training Loss: 0.989462
  Epoch 1 (valid) took 6.79s [processed 13113 samples/second]
 Validation:  Loss: 1.043493 | MRR: 0.209759
  Best result so far -- saved model as '/home/dev/resources/saved_models/neuralbowmodel-2020-01-06-14-14-06_model_best.pkl.gz'.
2020-01-06 14:24:47.722937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-06 14:24:47.722990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-06 14:24:47.723003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-06 14:24:47.723039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-06 14:24:47.723166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7191 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Test-All MRR (bs=1,000):  0.034
FuncNameTest-All MRR (bs=1,000):  0.031
Validation-All MRR (bs=1,000):  0.040
Test-java MRR (bs=1,000):  0.015
FuncNameTest-java MRR (bs=1,000):  0.032
Validation-java MRR (bs=1,000):  0.016
Test-ruby MRR (bs=1,000):  0.025
FuncNameTest-ruby MRR (bs=1,000):  0.021
Validation-ruby MRR (bs=1,000):  0.026
Test-go MRR (bs=1,000):  0.135
FuncNameTest-go MRR (bs=1,000):  0.037
Validation-go MRR (bs=1,000):  0.140
Test-javascript MRR (bs=1,000):  0.019
FuncNameTest-javascript MRR (bs=1,000):  0.009
Validation-javascript MRR (bs=1,000):  0.019
Test-python MRR (bs=1,000):  0.028
FuncNameTest-python MRR (bs=1,000):  0.020
Validation-python MRR (bs=1,000):  0.024
Test-php MRR (bs=1,000):  0.007
FuncNameTest-php MRR (bs=1,000):  0.031
Validation-php MRR (bs=1,000):  0.010

wandb: Waiting for W&B process to finish, PID 29
wandb: Program ended successfully.
wandb: Run summary:
wandb:                                 _runtime 1282.128327846527
wandb:                                    _step 26
wandb:                               _timestamp 1578321326.2116387
wandb:                               train-loss 0.9894618021117316
wandb:                                train-mrr 0.5125833560519748
wandb:                                  val-mrr 0.20975903474614863
wandb:                             val-time-sec 6.786708831787109
wandb:                                    epoch 1
wandb:                           train-time-sec 18.408169984817505
wandb:                                 val-loss 1.043493486522289
wandb:                        best_val_mrr_loss 1.043493486522289
wandb:                             best_val_mrr 0.20975903474614863
wandb:                               best_epoch 1
wandb:                  Test-All MRR (bs=1,000) 0.033967995739477314
wandb:          FuncNameTest-All MRR (bs=1,000) 0.030762230047009592
wandb:            Validation-All MRR (bs=1,000) 0.03960081690646877
wandb:                 Test-java MRR (bs=1,000) 0.015077754547884821
wandb:         FuncNameTest-java MRR (bs=1,000) 0.03163658674537041
wandb:           Validation-java MRR (bs=1,000) 0.01638225257871264
wandb:                 Test-ruby MRR (bs=1,000) 0.024962492687529692
wandb:         FuncNameTest-ruby MRR (bs=1,000) 0.021348530101276814
wandb:           Validation-ruby MRR (bs=1,000) 0.026130756374631865
wandb:                   Test-go MRR (bs=1,000) 0.13528512137119078
wandb:           FuncNameTest-go MRR (bs=1,000) 0.0365266531837107
wandb:             Validation-go MRR (bs=1,000) 0.1397268807939368
wandb:           Test-javascript MRR (bs=1,000) 0.01872516896307062
wandb:   FuncNameTest-javascript MRR (bs=1,000) 0.008637914207264874
wandb:     Validation-javascript MRR (bs=1,000) 0.018971572180009732
wandb:               Test-python MRR (bs=1,000) 0.028098753751330016
wandb:       FuncNameTest-python MRR (bs=1,000) 0.019523482574228212
wandb:         Validation-python MRR (bs=1,000) 0.024476468476160966
wandb:                  Test-php MRR (bs=1,000) 0.007435177502479026
wandb:          FuncNameTest-php MRR (bs=1,000) 0.03089330039791041
wandb:            Validation-php MRR (bs=1,000) 0.010044941117545501
wandb: Syncing files in wandb/run-20200106_141406-21aolhym:
wandb:   neuralbowmodel-2020-01-06-14-14-06-graph.pbtxt
wandb:   neuralbowmodel-2020-01-06-14-14-06.train_log
wandb:   neuralbowmodel-2020-01-06-14-14-06_model_best.pkl.gz
wandb: plus 9 W&B file(s) and 0 media file(s)
wandb:                                                                                
wandb: Synced neuralbowmodel-2020-01-06-14-14-06: https://app.wandb.ai/laksh47/CodeSearchNet/runs/21aolhym
root@daedalus:/home/dev/src# wandb off
W&B disabled, running your script from this directory will only write metadata locally.
root@daedalus:/home/dev/src# 
